{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dn-6c02VmqiN"
   },
   "outputs": [],
   "source": [
    "# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n",
    "# This will require you doing a lot of data preprocessing because\n",
    "# the dataset isn't split into training and validation for you\n",
    "# This code block has all the required inputs\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "3sd9dQWa23aj",
    "outputId": "fc947e10-ab8f-4387-c700-c9497e466931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-28 18:20:51--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n",
      "Resolving download.microsoft.com (download.microsoft.com)... 104.87.173.67, 2a02:26f0:fd:191::e59, 2a02:26f0:fd:1af::e59\n",
      "Connecting to download.microsoft.com (download.microsoft.com)|104.87.173.67|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 824894548 (787M) [application/octet-stream]\n",
      "Saving to: ‘/tmp/cats-and-dogs.zip’\n",
      "\n",
      "/tmp/cats-and-dogs. 100%[===================>] 786.68M  2.47MB/s    in 6m 7s   \n",
      "\n",
      "2019-08-28 18:26:58 (2.14 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code block downloads the full Cats-v-Dogs dataset and stores it as \n",
    "# cats-and-dogs.zip. It then unzips it to /tmp\n",
    "# which will create a tmp/PetImages directory containing subdirectories\n",
    "# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n",
    "# If the URL doesn't work, \n",
    "# .   visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
    "# And right click on the 'Download Manually' link to get a new URL\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
    "    -O \"/tmp/cats-and-dogs.zip\"\n",
    "\n",
    "local_zip = '/tmp/cats-and-dogs.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "gi3yD62a6X3S",
    "outputId": "7570346c-51ac-48e5-bcb7-c958a64e02ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n"
     ]
    }
   ],
   "source": [
    "# print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
    "# print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
    "# print(os.path.getsize('/tmp/PetImages/Dog/'))\n",
    "# # Expected Output:\n",
    "# 12501\n",
    "# 12501\n",
    "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
    "print(len(os.listdir('/tmp/PetImages/Dog/')))\n",
    "\n",
    "# Expected Output:\n",
    "# 12501\n",
    "# 12501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-QkLjxpmyK2"
   },
   "outputs": [],
   "source": [
    "# Use os.mkdir to create your directories\n",
    "# You will need a directory for cats-v-dogs, and subdirectories for training\n",
    "# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n",
    "try:\n",
    "    #YOUR CODE GOES HERE\n",
    "    os.mkdir('/tmp/cats-v-dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
    "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
    "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
    "    \n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "id": "zvSODo0f9LaU",
    "outputId": "7828a15a-fe37-42f2-bb53-314aa93d671b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666.jpg is zero length, so ignoring\n",
      "11702.jpg is zero length, so ignoring\n"
     ]
    }
   ],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a SOURCE directory containing the files\n",
    "# a TRAINING directory that a portion of the files will be copied to\n",
    "# a TESTING directory that a portion of the files will be copie to\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n",
    "# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n",
    "# and 10% of the images will be copied to the TESTING dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "#\n",
    "# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n",
    "# os.path.getsize(PATH) gives you the size of the file\n",
    "# copyfile(source, destination) copies a file from source to destination\n",
    "# random.sample(list, len(list)) shuffles a list\n",
    "\n",
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    pics = os.listdir(SOURCE)\n",
    "      \n",
    "    # those with len 0 are out\n",
    "    for pic in pics: \n",
    "        if os.path.getsize(SOURCE + pic) == 0 : \n",
    "            pics.remove(pic)\n",
    "            print('{0:<4s} is zero length, so ignoring'.format(pic))\n",
    "            \n",
    "    \n",
    "    # random permutation\n",
    "    random.sample(pics, len(pics))\n",
    "    \n",
    "    # train test split\n",
    "    end = int(len(pics) * SPLIT_SIZE)\n",
    "    train = pics[:end]\n",
    "    test = pics[end :]\n",
    "    \n",
    "    # copying images\n",
    "    for pic in train :\n",
    "        \n",
    "        picture_path = os.path.join(SOURCE , pic)\n",
    "        copyfile(picture_path, TRAINING + pic)\n",
    "    \n",
    "    for pic in test: \n",
    "        picture_path = os.path.join(SOURCE , pic)\n",
    "        copyfile(picture_path, TESTING + pic )\n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
    "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
    "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
    "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
    "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
    "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
    "\n",
    "split_size = .9\n",
    "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
    "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
    "\n",
    "# Expected output\n",
    "# 666.jpg is zero length, so ignoring\n",
    "# 11702.jpg is zero length, so ignoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luthalB76ufC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11250\n",
      "11250\n",
      "1250\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
    "print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
    "print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
    "print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
    "\n",
    "# Expected output:\n",
    "# 11250\n",
    "# 11250\n",
    "# 1250\n",
    "# 1250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BQrav4anTmj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0828 22:04:43.292188 140665494185792 deprecation.py:506] From /home/kuba/venv/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0828 22:04:43.645532 140665494185792 deprecation.py:323] From /home/kuba/venv/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               9470464   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 9,494,561\n",
      "Trainable params: 9,494,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
    "# USE AT LEAST 3 CONVOLUTION LAYERS\n",
    "model = tf.keras.models.Sequential([\n",
    "# YOUR CODE HERE\n",
    "        Conv2D(16, (3,3), activation = 'relu' , input_shape = (150, 150,3)),\n",
    "        MaxPooling2D(2,2), \n",
    "        Conv2D(32, (3,3), activation = 'relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(64, (3,3), activation = 'relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation = 'relu'),\n",
    "        Dense(1, activation = 'sigmoid')\n",
    "        \n",
    "])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlNjoJ5D61N6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22498 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = '/tmp/cats-v-dogs/training/'\n",
    "train_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
    "train_generator = train_datagen.flow_from_directory(   TRAINING_DIR, \n",
    "                                                       batch_size = 100,\n",
    "                                                       class_mode = 'binary',\n",
    "                                                       target_size = (150,150))\n",
    "\n",
    "VALIDATION_DIR = '/tmp/cats-v-dogs/testing/'\n",
    "validation_datagen = ImageDataGenerator(rescale= 1.0/255)\n",
    "validation_generator = train_datagen.flow_from_directory(  VALIDATION_DIR, \n",
    "                                                           batch_size = 100,\n",
    "                                                           class_mode = 'binary',\n",
    "                                                           target_size = (150,150))\n",
    "\n",
    "\n",
    "\n",
    "# Expected Output:\n",
    "# Found 22498 images belonging to 2 classes.\n",
    "# Found 2500 images belonging to 2 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyS4n53w7DxC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 29/100 [=======>......................] - ETA: 1:15 - loss: 0.6943 - acc: 0.5414"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8d7105b9e131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                               validation_data=validation_generator)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# history = model.fit_generator(train_generator,\n",
    "#                               epochs=15,\n",
    "#                               verbose=1,\n",
    "#                               validation_data=validation_generator)\n",
    "\n",
    "# # The expectation here is that the model will train, and that accuracy will be > 95% on both training and validation\n",
    "# # i.e. acc:A1 and val_acc:A2 will be visible, and both A1 and A2 will be > .9\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=50,\n",
    "                              steps_per_epoch = 100,\n",
    "                              verbose=1,\n",
    "                              validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWZrJN4-65RC"
   },
   "outputs": [],
   "source": [
    "# PLOT LOSS AND ACCURACY\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
    "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
    "\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "# Desired output. Charts with training and validation metrics. No crash :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqL6FYUrtXpf"
   },
   "outputs": [],
   "source": [
    "# Here's a codeblock just for fun. You should be able to upload an image here \n",
    "# and have it classified without crashing\n",
    "\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    " \n",
    "  # predicting images\n",
    "  path = '/content/' + fn\n",
    "  img = image.load_img(path, target_size=(# YOUR CODE HERE))\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "\n",
    "  images = np.vstack([x])\n",
    "  classes = model.predict(images, batch_size=10)\n",
    "  print(classes[0])\n",
    "  if classes[0]>0.5:\n",
    "    print(fn + \" is a dog\")\n",
    "  else:\n",
    "    print(fn + \" is a cat\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 5 - Question.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
